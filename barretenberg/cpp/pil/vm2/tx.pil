include "public_inputs.pil";
include "execution.pil";
include "precomputed.pil";
include "trees/note_hash_tree_check.pil";
include "poseidon2_hash.pil";
include "tx_context.pil";
include "tx_discard.pil";

// Refer to https://excalidraw.com/#json=XcT7u7Ak5rZhWqT51KwTW,VPI-Q1D7hW8_lYhf6V4bNg for a visual guide to the tx trace.
// The tx trace manages the various phases that a transaction will undergo while it is being executed.
// These different phases for a tx include:
// (a) Non-Revertible Private Insertion of nullifiers, note hashes and L2 to L1 Messages
// (b) Non-Revertible SETUP phase
// (c) Revertible Private insertion of nullifiers, note hashes and L2 to L1 Messages
// (d) Revertible APPLOGIC phase
// (e) Revertible TEARDOWN phase
// (f) Non-Revertible Collect Gas Phase

// Things that are still underconstrained - see issue tracker #14666
namespace tx;

    pol commit sel;
    sel * (1 - sel) = 0;
    #[skippable_if]
    sel = 0;

    // Lifecycle constraints: we want all executions of the AVM to have exactly one run of the tx trace
    // and no extraneous rows in the tx trace

    // Can only turn on sel after the first row
    #[NO_EXTRANEOUS_ROWS]
    (1 - sel) * sel' * (1 - precomputed.first_row) = 0;
    // You MUST turn on sel after the first row
    #[SEL_ON_FIRST_ROW]
    precomputed.first_row * (1 - sel') = 0;

    // Sel can only become off after the last phase (cleanup)
    #[NO_EARLY_END]
    sel * (1 - sel') * (1 - is_cleanup) = 0;

    pol commit start_tx;
    #[START_WITH_SEL]
    start_tx' = (1 - sel) * sel';

    pol commit phase_value;
    // if is_padded = 1, this is a padded row
    // A padded row exists for a phase (refer to a to f above for the phases) that is unused for this transaction
    // This means that the length of values to read for it is 0. We still need to perform some check to prevent
    // malicious skipping of valid phases.
    pol commit is_padded;
    is_padded * (1 - is_padded) = 0;
    // A padded row can't revert
    is_padded * reverted = 0;

    pol commit start_phase;
    start_phase * (1 - start_phase) = 0;
    pol commit end_phase;
    end_phase * (1 - end_phase) = 0;

    pol NOT_LAST = sel' * sel; // Could probably use a different condition (i.e. COLLECT_GAS is last row - save this for optimisation)
    pol NOT_PHASE_END = NOT_LAST * (1 - end_phase);

    #[START_FOLLOWS_END]
    NOT_LAST * (start_phase' - (end_phase + precomputed.first_row)) = 0;

    // end_phase = 1, remaining_phase - 1 = 0;
    pol commit is_revertible;
    pol commit reverted;
    // phase_value stays the same unless we are at end_phase or reverted
    #[PHASE_VALUE_CONTINUITY]
    NOT_PHASE_END * (1 - reverted) * (1 - precomputed.first_row) * (phase_value' - phase_value) = 0;
    // At the start of a new phase, we need to increment the phase value
    #[INCR_PHASE_VALUE_ON_END]
    NOT_LAST * (1 - reverted) * end_phase * (phase_value' - (phase_value + 1)) = 0;

    // If reverted == 1, is_revertible must be 1
    reverted * (1 - is_revertible) = 0;

    // Control flow
    pol commit read_pi_offset; // index in public inputs to read data from -> Revisit!
    pol commit read_pi_length_offset; // index in public inputs to read lengths from
    pol commit write_pi_offset; // index in public inputs to write data to: Constrain in the phases that need it.

    pol commit remaining_phase_inv;
    pol commit remaining_phase_counter;
    pol commit remaining_phase_minus_one_inv;
    // If remaining_phase_counter == 0, is_padded = 1
    // Leaves end_phase unconstrained but the prover has no choice since if he doesn't activate end_phase
    // remaining_phase_counter will become p-1 and he'll never be able to complete a run.
    #[REM_COUNT_IS_ZERO]
    sel * (remaining_phase_counter * (is_padded * (1 - remaining_phase_inv) + remaining_phase_inv) - 1 + is_padded) = 0;

    pol REM_COUNT_MINUS_1 = remaining_phase_counter - 1;
    // If remaining_phase_counter == 1, end_phase = 1
    #[REM_COUNT_IS_ONE]
    sel * (1 - is_padded) * (REM_COUNT_MINUS_1 * (end_phase * (1 - remaining_phase_minus_one_inv) + remaining_phase_minus_one_inv) - 1 + end_phase) = 0;

    // TODO: Add constraints to propagate down the things read from precomputed
    // while NOT_PHASE_END
    #[READ_PHASE_TABLE]
    start_phase {
        phase_value,
        is_public_call_request,
        is_collect_fee,
        is_tree_padding,
        is_cleanup,

        is_revertible,
        read_pi_offset,
        read_pi_length_offset,
        sel_non_revertible_append_note_hash,
        sel_non_revertible_append_nullifier,
        sel_non_revertible_append_l2_l1_msg,
        sel_revertible_append_note_hash,
        sel_revertible_append_nullifier,
        sel_revertible_append_l2_l1_msg,
        sel_can_emit_note_hash,
        sel_can_emit_nullifier,
        sel_can_write_public_data,
        sel_can_emit_unencrypted_log,
        sel_can_emit_l2_l1_msg
    }
    in
    precomputed.sel_phase {
        precomputed.phase_value,
        precomputed.is_public_call_request_phase,
        precomputed.sel_collect_fee,
        precomputed.sel_tree_padding,
        precomputed.sel_cleanup,

        precomputed.is_revertible,
        precomputed.read_public_input_offset,
        precomputed.read_public_input_length_offset,
        precomputed.sel_non_revertible_append_note_hash,
        precomputed.sel_non_revertible_append_nullifier,
        precomputed.sel_non_revertible_append_l2_l1_msg,
        precomputed.sel_revertible_append_note_hash,
        precomputed.sel_revertible_append_nullifier,
        precomputed.sel_revertible_append_l2_l1_msg,
        precomputed.sel_can_emit_note_hash,
        precomputed.sel_can_emit_nullifier,
        precomputed.sel_can_write_public_data,
        precomputed.sel_can_emit_unencrypted_log,
        precomputed.sel_can_emit_l2_l1_msg
    };

    // If a phase reverts (when it is allowed to), it jumps to a known next phase
    #[PHASE_JUMP_ON_REVERT]
    reverted { phase_value, phase_value' } in
    precomputed.sel_phase { precomputed.phase_value, precomputed.next_phase_on_revert};

    // === Phase Lengths ===
    // Read the number of steps in the phase from public inputs

    pol IS_ONE_SHOT_PHASE = is_collect_fee + is_tree_padding + is_cleanup;

    pol commit sel_read_phase_length;
    // Read length if start phase and not one shot phase
    #[READ_PI_LENGTH_SEL]
    sel * (sel_read_phase_length - start_phase * (1 - IS_ONE_SHOT_PHASE)) = 0;
    #[READ_PHASE_LENGTH]
    sel_read_phase_length { read_pi_length_offset, remaining_phase_counter } in
    public_inputs.sel { precomputed.clk, public_inputs.cols[0] };

    // In one shot phases, remaining phase counter has to be 1
    #[ONE_SHOT_REMAINING_PHASE_COUNTER_ONE]
    IS_ONE_SHOT_PHASE * (remaining_phase_counter - 1) = 0;

    // Phase events decrement at each row
    #[DECR_REM_PHASE_EVENTS]
    (1 - precomputed.first_row) * NOT_PHASE_END * (remaining_phase_counter' - (remaining_phase_counter - 1)) = 0;

    // The read index increments at each row
    #[INCR_READ_PI_OFFSET]
    (1 - precomputed.first_row) * NOT_PHASE_END * (read_pi_offset' - (read_pi_offset + 1)) = 0;

    // === Public Call Request Phase ===
    pol commit is_public_call_request;
    pol commit msg_sender;
    pol commit contract_addr;
    pol commit is_static;
    pol commit calldata_hash;

    // Read information relating to the public call request from public inputs
    #[READ_PUBLIC_CALL_REQUEST_PHASE]
    is_public_call_request {
        read_pi_offset,
        msg_sender,
        contract_addr,
        is_static,
        calldata_hash
    } in
    public_inputs.sel {
        precomputed.clk,
        public_inputs.cols[0],
        public_inputs.cols[1],
        public_inputs.cols[2],
        public_inputs.cols[3]
    };

    pol commit should_process_call_request;
    should_process_call_request = is_public_call_request * (1 - is_padded);

    pol commit is_teardown_phase; // TODO: Constrain
    is_teardown_phase * (1 - is_teardown_phase) = 0;

    pol commit prev_l2_gas_used_sent_to_enqueued_call;
    pol commit prev_da_gas_used_sent_to_enqueued_call;
    pol commit next_l2_gas_used_sent_to_enqueued_call;
    pol commit next_da_gas_used_sent_to_enqueued_call;

    // prev_gas_used_sent_to_enqueued_call = is_teardown_phase ? 0 : prev_gas_used
    should_process_call_request * ((0 - prev_l2_gas_used) * is_teardown_phase + prev_l2_gas_used - prev_l2_gas_used_sent_to_enqueued_call) = 0;
    should_process_call_request * ((0 - prev_da_gas_used) * is_teardown_phase + prev_da_gas_used - prev_da_gas_used_sent_to_enqueued_call) = 0;
    // next_gas_used = is_teardown_gas_phase ? prev_gas_used : next_gas_used_sent_to_enqueued_call
    // In the teardown phase, next_gas_used_sent_to_enqueued_call is unconstrained on purpose: we don't care about gas usage in teardown
    should_process_call_request * ((prev_l2_gas_used - next_l2_gas_used_sent_to_enqueued_call) * is_teardown_phase + next_l2_gas_used_sent_to_enqueued_call - next_l2_gas_used) = 0;
    should_process_call_request * ((prev_da_gas_used - next_da_gas_used_sent_to_enqueued_call) * is_teardown_phase + next_da_gas_used_sent_to_enqueued_call - next_da_gas_used) = 0;

    // Public Call Requests are dispatch to the execution trace
    // We match the values to the start of an enqueued call in the execution trace
    #[DISPATCH_EXEC_START]
    should_process_call_request {
        next_context_id,
        discard,
        msg_sender,
        contract_addr,
        fee,
        is_static,
        // Tree State
        prev_note_hash_tree_root,
        prev_note_hash_tree_size,
        prev_num_note_hashes_emitted,
        prev_nullifier_tree_root,
        prev_nullifier_tree_size,
        prev_num_nullifiers_emitted,
        prev_public_data_tree_root,
        prev_public_data_tree_size,
        prev_written_public_data_slots_tree_root,
        prev_written_public_data_slots_tree_size,
        l1_l2_tree_root,
        // Side Effect States
        prev_num_unencrypted_logs,
        prev_num_l2_to_l1_messages,
        // Gas Info
        prev_l2_gas_used_sent_to_enqueued_call,
        prev_da_gas_used_sent_to_enqueued_call,
        l2_gas_limit,
        da_gas_limit
    } in
    execution.enqueued_call_start {
        execution.context_id, // next_context_id must be constrained in the execution trace on enqueued_call_start
        execution.discard,
        execution.msg_sender,
        execution.contract_address,
        execution.transaction_fee,
        execution.is_static,
        // Tree State
        execution.prev_note_hash_tree_root,
        execution.prev_note_hash_tree_size,
        execution.prev_num_note_hashes_emitted,
        execution.prev_nullifier_tree_root,
        execution.prev_nullifier_tree_size,
        execution.prev_num_nullifiers_emitted,
        execution.prev_public_data_tree_root,
        execution.prev_public_data_tree_size,
        execution.prev_written_public_data_slots_tree_root,
        execution.prev_written_public_data_slots_tree_size,
        execution.l1_l2_tree_root,
        // Side Effect States
        execution.prev_num_unencrypted_logs,
        execution.prev_num_l2_to_l1_messages,
        // Gas Info
        execution.prev_l2_gas_used,
        execution.prev_da_gas_used,
        execution.l2_gas_limit,
        execution.da_gas_limit
    };

    // We retrieve the return values at the end of an enqueued call in the execution trace
    #[DISPATCH_EXEC_END]
    should_process_call_request {
       next_context_id,
       next_context_id', // The context id of the next row must be the one after the execution of the enqueued call
       reverted,
       discard,
       // Tree State
       next_note_hash_tree_root,
       next_note_hash_tree_size,
       next_num_note_hashes_emitted,
       next_nullifier_tree_root,
       next_nullifier_tree_size,
       next_num_nullifiers_emitted,
       next_public_data_tree_root,
       next_public_data_tree_size,
       next_written_public_data_slots_tree_root,
       next_written_public_data_slots_tree_size,
       l1_l2_tree_root,
       // Side Effect States
       next_num_unencrypted_logs,
       next_num_l2_to_l1_messages,
       // Gas Info
       next_l2_gas_used_sent_to_enqueued_call,
       next_da_gas_used_sent_to_enqueued_call
    } in
    execution.enqueued_call_end {
       execution.context_id,
       execution.next_context_id,
       execution.sel_failure,
       execution.discard,
       // Tree State
       execution.note_hash_tree_root,
       execution.note_hash_tree_size,
       execution.num_note_hashes_emitted,
       execution.nullifier_tree_root,
       execution.nullifier_tree_size,
       execution.num_nullifiers_emitted,
       execution.public_data_tree_root,
       execution.public_data_tree_size,
       execution.written_public_data_slots_tree_root,
       execution.written_public_data_slots_tree_size,
       execution.l1_l2_tree_root,
       // Side Effect States
       execution.num_unencrypted_logs,
       execution.num_l2_to_l1_messages,
       // Gas Info
       execution.l2_gas_used,
       execution.da_gas_used
    };

    // === PRIVATE SIDE EFFECT INSERTIONS ===
    pol commit sel_revertible_append_note_hash;
    pol commit sel_non_revertible_append_note_hash;
    pol commit sel_revertible_append_nullifier;
    pol commit sel_non_revertible_append_nullifier;
    pol commit sel_revertible_append_l2_l1_msg;
    pol commit sel_non_revertible_append_l2_l1_msg;
    // A tree selector means we need to get the tree value
    pol commit is_tree_insert_phase;
    is_tree_insert_phase  = sel_revertible_append_note_hash + sel_non_revertible_append_note_hash + sel_revertible_append_nullifier + sel_non_revertible_append_nullifier;
    pol commit leaf_value;
    // Shared column to track the inverse of the remaining side effects for note hashes, nullifiers and l2 to l1 messages
    pol commit remaining_side_effects_inv;

    // TODO: Maybe unify with other public inputs reads?
    // Read the tree value from public inputs
    #[READ_TREE_INSERT_VALUE]
    is_tree_insert_phase { read_pi_offset, leaf_value } in
    public_inputs.sel { precomputed.clk, public_inputs.cols[0] };

    // ===== NOTE HASHES =====
    pol commit should_try_note_hash_append;
    should_try_note_hash_append = sel * (1 - is_padded) * (sel_revertible_append_note_hash + sel_non_revertible_append_note_hash);

    // If we are at the maximum emitted note hashes, we must revert
    pol REMAINING_NOTE_HASH_WRITES = constants.MAX_NOTE_HASHES_PER_TX -
        prev_num_note_hashes_emitted;
    // Revert if remaining note hashes is 0
    #[MAX_NOTE_HASH_WRITES_REACHED]
    should_try_note_hash_append * (REMAINING_NOTE_HASH_WRITES * (reverted * (1 - remaining_side_effects_inv) + remaining_side_effects_inv) - 1 + reverted) = 0;

    // Commited since it's used in the lookup
    pol commit should_note_hash_append;
    should_try_note_hash_append * ((1 - reverted) - should_note_hash_append) = 0;

    #[NOTE_HASH_APPEND]
    should_note_hash_append {
        leaf_value,
        prev_note_hash_tree_size,
        prev_note_hash_tree_root,
        precomputed.zero,
        sel_revertible_append_note_hash,
        prev_num_note_hashes_emitted,
        discard, // from tx_discard.pil virtual trace
        next_note_hash_tree_root
    } in
    note_hash_tree_check.write {
        note_hash_tree_check.note_hash,
        note_hash_tree_check.leaf_index,
        note_hash_tree_check.prev_root,
        note_hash_tree_check.should_silo,
        note_hash_tree_check.should_unique,
        note_hash_tree_check.note_hash_index,
        note_hash_tree_check.discard,
        note_hash_tree_check.next_root
    };

    // If reverted, next state is unconstrained. This should be ok since we are going to restore the end of setup or fail proving.
    should_note_hash_append * (prev_note_hash_tree_size + 1 - next_note_hash_tree_size) = 0;
    should_note_hash_append * (prev_num_note_hashes_emitted + 1 - next_num_note_hashes_emitted) = 0;

    // ===== NULLIFIERS =====
    pol commit should_try_nullifier_append;
    should_try_nullifier_append = sel * (1 - is_padded) * (sel_revertible_append_nullifier + sel_non_revertible_append_nullifier);

    // Commited since it's used in the lookup
    pol commit should_nullifier_append;
    pol NULLIFIER_LIMIT_ERROR = 1 - should_nullifier_append;

    // If we are at the maximum emitted nullifiers, we must disable the lookup
    pol REMAINING_NULLIFIER_WRITES = constants.MAX_NULLIFIERS_PER_TX -
        prev_num_nullifiers_emitted;
    // Disable the lookup if remaining nullifiers is 0
    #[MAX_NULLIFIER_WRITES_REACHED]
    should_try_nullifier_append * (REMAINING_NULLIFIER_WRITES * (NULLIFIER_LIMIT_ERROR * (1 - remaining_side_effects_inv) + remaining_side_effects_inv) - 1 + NULLIFIER_LIMIT_ERROR) = 0;

    // If over the limit, reverted=1, else, reverted is constrained by the lookup
    should_try_nullifier_append * NULLIFIER_LIMIT_ERROR * (1 - reverted) = 0;

    #[NULLIFIER_APPEND]
    should_nullifier_append {
        reverted,
        leaf_value,
        prev_nullifier_tree_root,
        next_nullifier_tree_root,
        prev_nullifier_tree_size,
        discard, // from tx_discard.pil virtual trace
        prev_num_nullifiers_emitted,
        precomputed.zero
    } in
    nullifier_check.write {
        nullifier_check.exists,
        nullifier_check.nullifier,
        nullifier_check.root,
        nullifier_check.write_root,
        nullifier_check.tree_size_before_write,
        nullifier_check.discard,
        nullifier_check.nullifier_index,
        nullifier_check.should_silo
    };

    // If reverted, next state is unconstrained. This should be ok since we are going to restore the end of setup or fail proving.
    should_nullifier_append * (1 - reverted) * (prev_nullifier_tree_size + 1 - next_nullifier_tree_size) = 0;
    should_nullifier_append * (1 - reverted) * (prev_num_nullifiers_emitted + 1 - next_num_nullifiers_emitted) = 0;

    // ===== L2 - L1 Messages =====
    pol commit should_try_l2_l1_msg_append;
    should_try_l2_l1_msg_append = sel * (1 - is_padded) * (sel_revertible_append_l2_l1_msg + sel_non_revertible_append_l2_l1_msg);

    pol commit l2_l1_msg_contract_address;
    pol commit l2_l1_msg_recipient;
    pol commit l2_l1_msg_content;

    #[READ_L2_L1_MSG]
    should_try_l2_l1_msg_append { read_pi_offset, l2_l1_msg_recipient, l2_l1_msg_content, l2_l1_msg_contract_address } in
    public_inputs.sel { precomputed.clk, public_inputs.cols[0], public_inputs.cols[1], public_inputs.cols[2] };

    // If we are at the maximum emitted messages, we must revert
    pol REMAINING_L2_TO_L1_MSG_WRITES = constants.MAX_L2_TO_L1_MSGS_PER_TX -
        prev_num_l2_to_l1_messages;
    // Revert if REMAINING_L2_TO_L1_MSG_WRITES is 0
    #[MAX_L2_L1_MSG_WRITES_REACHED]
    should_try_l2_l1_msg_append * (1 - is_padded) * (REMAINING_L2_TO_L1_MSG_WRITES * (reverted * (1 - remaining_side_effects_inv) + remaining_side_effects_inv) - 1 + reverted) = 0;

    pol commit should_l2_l1_msg_append;
    // A msg emit must be written to PI if it didn't revert, and is not discard.
    should_try_l2_l1_msg_append * ((1 - reverted) * (1 - discard) - should_l2_l1_msg_append) = 0;
    // discard is from tx_discard.pil virtual trace

    should_l2_l1_msg_append * (constants.AVM_PUBLIC_INPUTS_AVM_ACCUMULATED_DATA_L2_TO_L1_MSGS_ROW_IDX + prev_num_l2_to_l1_messages - write_pi_offset) = 0;

    #[WRITE_L2_L1_MSG]
    should_l2_l1_msg_append { write_pi_offset, l2_l1_msg_recipient, l2_l1_msg_content, l2_l1_msg_contract_address } in
    public_inputs.sel { precomputed.clk, public_inputs.cols[0], public_inputs.cols[1], public_inputs.cols[2] };

    // If reverted, next count is unconstrained. This should be ok since we are going to restore the end of setup or fail proving.
    #[UPDATE_NUM_L2_TO_L1_MSGS]
    should_try_l2_l1_msg_append * (1 - reverted) * (prev_num_l2_to_l1_messages + 1 - next_num_l2_to_l1_messages) = 0;

    // Collect Gas
    pol commit is_collect_fee;
    pol commit effective_fee_per_da_gas;
    pol commit effective_fee_per_l2_gas;

    #[READ_EFFECTIVE_FEE_PUBLIC_INPUTS]
    is_collect_fee {
        read_pi_offset,
        effective_fee_per_da_gas,
        effective_fee_per_l2_gas
    } in
    public_inputs.sel {
        precomputed.clk,
        public_inputs.cols[0],
        public_inputs.cols[1]
    };

    pol commit fee_payer;
    pol commit fee_payer_pi_offset; // TODO: remove when we can use constants in lookups
    fee_payer_pi_offset = is_collect_fee * constants.AVM_PUBLIC_INPUTS_FEE_PAYER_ROW_IDX;

    #[READ_FEE_PAYER_PUBLIC_INPUTS]
    is_collect_fee {
        fee_payer_pi_offset,
        fee_payer
    } in
    public_inputs.sel {
        precomputed.clk,
        public_inputs.cols[0]
    };

    pol commit fee;

    // Overflow safe since effective_fee_per_gas is 128 bits, and gas used is 32 bits, making fee at most 161 bits.
    #[COMPUTE_FEE]
    is_collect_fee * (effective_fee_per_da_gas * prev_da_gas_used + effective_fee_per_l2_gas * prev_l2_gas_used - fee) = 0;
    // During teardown, transaction fee must be the same as the one used in collect_fee (the following row/phase)
    // (unless it is a padded row)
    #[TEARDOWN_GETS_FEE]
    is_teardown_phase * (1 - is_padded) * (fee' - fee) = 0;
    // In all other phases, force fee to 0
    #[FEE_ZERO_UNLESS_COLLECT_FEE_OR_TEARDOWN]
    (1 - is_collect_fee) * (1 - is_teardown_phase) * fee = 0;

    pol commit fee_juice_contract_address;
    is_collect_fee * (constants.FEE_JUICE_ADDRESS - fee_juice_contract_address) = 0;
    pol commit fee_juice_balances_slot;
    is_collect_fee * (constants.FEE_JUICE_BALANCES_SLOT - fee_juice_balances_slot) = 0;

    pol commit fee_juice_balance_slot;

    #[BALANCE_SLOT_POSEIDON2]
    is_collect_fee { is_collect_fee, fee_juice_balances_slot, fee_payer, precomputed.zero, fee_juice_balance_slot }
    in poseidon2_hash.end { poseidon2_hash.start, poseidon2_hash.input_0, poseidon2_hash.input_1, poseidon2_hash.input_2, poseidon2_hash.output };

    pol commit fee_payer_balance;

    #[BALANCE_READ]
    is_collect_fee {
       fee_payer_balance,
       fee_juice_contract_address,
       fee_juice_balance_slot,
       prev_public_data_tree_root
    } in public_data_check.sel {
       public_data_check.value,
       public_data_check.address,
       public_data_check.slot,
       public_data_check.root
    };

    #[BALANCE_VALIDATION]
    is_collect_fee { fee, fee_payer_balance, precomputed.zero }
    in ff_gt.sel_gt { ff_gt.a, ff_gt.b, ff_gt.result };

    pol commit fee_payer_new_balance;
    is_collect_fee * (fee_payer_balance - fee - fee_payer_new_balance) = 0;

    pol commit uint32_max;
    is_collect_fee * (uint32_max - 0xffffffff) = 0;

    // TODO: Lookup for now: will need multipermutation
    #[BALANCE_UPDATE]
    is_collect_fee {
        fee_payer_new_balance,
        fee_juice_contract_address,
        fee_juice_balance_slot,
        prev_public_data_tree_root,
        next_public_data_tree_root,
        prev_public_data_tree_size,
        next_public_data_tree_size,
        uint32_max
    } in public_data_check.write {
        public_data_check.value,
        public_data_check.address,
        public_data_check.slot,
        public_data_check.root,
        public_data_check.write_root,
        public_data_check.tree_size_before_write,
        public_data_check.tree_size_after_write,
        public_data_check.clk
    };

    is_collect_fee * (constants.AVM_PUBLIC_INPUTS_TRANSACTION_FEE_ROW_IDX - write_pi_offset) = 0;

    #[WRITE_FEE_PUBLIC_INPUTS]
    is_collect_fee { write_pi_offset, fee }
    in public_inputs.sel { precomputed.clk, public_inputs.cols[0] };

    // ===== TREE PADDING =====
    pol commit is_tree_padding;

    // Roots and num emitted don't change, only size changes.
    // Necessary since we are allowed in this phase to change tree roots, sizes and emitted counts.
    #[NOTE_HASH_TREE_ROOT_IMMUTABLE_IN_PADDING]
    is_tree_padding * (prev_note_hash_tree_root - next_note_hash_tree_root) = 0;
    #[PAD_NOTE_HASH_TREE]
    is_tree_padding * ((prev_note_hash_tree_size + constants.MAX_NOTE_HASHES_PER_TX - prev_num_note_hashes_emitted) - next_note_hash_tree_size) = 0;
    #[NOTE_HASHES_EMITTED_IMMUTABLE_IN_PADDING]
    is_tree_padding * (prev_num_note_hashes_emitted - next_num_note_hashes_emitted) = 0;
    #[NULLIFIER_TREE_ROOT_IMMUTABLE_IN_PADDING]
    is_tree_padding * (prev_nullifier_tree_root - next_nullifier_tree_root) = 0;
    #[PAD_NULLIFIER_TREE]
    is_tree_padding * ((prev_nullifier_tree_size + constants.MAX_NULLIFIERS_PER_TX - prev_num_nullifiers_emitted) - next_nullifier_tree_size) = 0;
    #[NULLIFIERS_EMITTED_IMMUTABLE_IN_PADDING]
    is_tree_padding * (prev_num_nullifiers_emitted - next_num_nullifiers_emitted) = 0;

    // ===== CLEANUP =====
    pol commit is_cleanup;
