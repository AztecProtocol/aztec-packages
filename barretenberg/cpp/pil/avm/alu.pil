namespace alu(256);

    // ========= Table ALU-TR =================================================

    // References to main trace table of sub-operations, clk, intermediate
    // registers, operation selectors.
    // TODO: Think on optimizations to decrease the number of such "copied" columns
    pol commit clk;
    pol commit ia; // Intermediate registers
    pol commit ib;
    pol commit ic;
    pol commit op_add; // Operation selectors
    pol commit op_sub;
    pol commit op_mul;
    pol commit op_div;
    pol commit op_not;
    pol commit op_eq;
    pol commit op_cast;
    pol commit op_cast_prev; // Predicate on whether op_cast is enabled at previous row
    pol commit sel_alu; // Predicate to activate the copy of intermediate registers to ALU table.
    pol commit op_lt;
    pol commit op_lte;
    pol commit sel_cmp; // Predicate if LT or LTE is set
    pol commit sel_rng_chk; // Predicate representing a range check row.
    pol commit op_shl;
    pol commit op_shr;
    pol commit sel_shift_which; // Predicate if SHR or SHR is set

    // Instruction tag (1: u8, 2: u16, 3: u32, 4: u64, 5: u128, 6: field) copied from Main table
    pol commit in_tag;

    // Flattened boolean instruction tags
    pol commit ff_tag;
    pol commit u8_tag;
    pol commit u16_tag;
    pol commit u32_tag;
    pol commit u64_tag;
    pol commit u128_tag;

    // 8-bit slice registers
    pol commit u8_r0;
    pol commit u8_r1;

    // 16-bit slice registers
    pol commit u16_r0;
    pol commit u16_r1;
    pol commit u16_r2;
    pol commit u16_r3;
    pol commit u16_r4;
    pol commit u16_r5;
    pol commit u16_r6;
    pol commit u16_r7;
    pol commit u16_r8;
    pol commit u16_r9;
    pol commit u16_r10;
    pol commit u16_r11;
    pol commit u16_r12;
    pol commit u16_r13;
    pol commit u16_r14;

    // Carry flag
    pol commit cf;

    // Compute predicate telling whether there is a row entry in the ALU table.
    sel_alu = op_add + op_sub + op_mul + op_not + op_eq + op_cast + op_lt + op_lte + op_shr + op_shl + op_div;
    sel_cmp = op_lt + op_lte;
    sel_shift_which = op_shl + op_shr;

    // ========= Type Constraints =============================================
    // TODO: Range constraints
    //       - intermediate registers ia and ib (inputs) depending on flag (or inherited from previous ops?)
    //       - intermediate register ic (in some operations it might be inherited based on ia and ib ranges. To be checked.)
    // Remark: Operation selectors are constrained in the main trace.

    // cf is boolean
    cf * (1 - cf) = 0;

    // Boolean flattened instructions tags
    ff_tag * (1 - ff_tag) = 0;
    u8_tag * (1 - u8_tag) = 0;
    u16_tag * (1 - u16_tag) = 0;
    u32_tag * (1 - u32_tag) = 0;
    u64_tag * (1 - u64_tag) = 0;
    u128_tag * (1 - u128_tag) = 0;

    // Mutual exclusion of the flattened instruction tag.
    sel_alu * (ff_tag + u8_tag + u16_tag + u32_tag + u64_tag + u128_tag - 1) = 0;

    // Correct flattening of the instruction tag.
    in_tag = u8_tag + 2 * u16_tag + 3 * u32_tag + 4 * u64_tag + 5 * u128_tag + 6 * ff_tag;

    // Operation selectors are copied from main table and do not need to be constrained here.
    // Mutual exclusion of op_add and op_sub are derived from their mutual exclusion in the
    // main trace which is ensured by the operation decomposition.

    // ========= ARITHMETIC OPERATION - EXPLANATIONS =================================================
    // Main trick for arithmetic operations modulo 2^k is to perform the operation
    // over the integers and expressing the result as low + high * 2^k with low
    // smaller than 2^k. low is used as the output. This works as long this does
    // not underflow/overflow the underlying finite field order (u128 multiplication
    // will be handled differently). If we want to prove that c = OP(a,b) where OP
    // denotes an arithmetic operation modulo 2^k, we can achieve this with two relations:
    // (1) low + high * 2^k - OP'(a,b) = 0
    // (2) low - c = 0
    //
    // where OP' denotes the same operation as OP but over the integers (not mod 2^k).
    // We support u8, u16, u32, u64, u128 types and decompose low into
    // smaller bit slices, e.g., 16. For instance, low = s_0 + s_1 * 2^16 + s_2 * 2^32 + ...
    // The slices have to be range constrained and there is a trade-off between the
    // number of registers and complexity of the range constraints.
    //
    // Optimization: We will capture the relation (1) for all types under the same umbrella
    //               by re-using the decomposition used for u128 type for the lower types.
    //               This works because any larger power of 2^k divides 2^l whenever k <= l.
    //               To be able to express "low" for u8, we need a 8-bit limb for the lowest
    //               bits. A bit decomposition covering all types is:
    //  s8_0 + s8_1 * 2^8 + s16_0 * 2^16 + s16_1 * 2^32 ... + s16_6 * 2^112 + carry * 2^128 - OP'(a,b) = 0
    //               where s8_i's are 8-bit registers and s16's 16-bit registers.
    //               For type uk, we set low to the k-bit truncated part of register decomposition.
    //               As example, for u32: low = s8_0 + s8_1 * 2^8 + s16_0 * 2^16
    //               and for u8: low = s8_0
    //
    // TODO: It is open whether we might get efficiency gain to use larger registers for the higher
    //       parts of the bit decomposition.

    // ============= Helper polynomial terms ============================
    // These are intermediate polynomial terms which are not commited but
    // serves to an algebraic expression of commited polynomials in a more concise way.

    // Bit slices partial sums
    pol SUM_8   = u8_r0;
    pol SUM_16  = SUM_8      + u8_r1 * 2**8;
    pol SUM_32  = SUM_16     + u16_r0 * 2**16;
    pol SUM_64  = SUM_32     + u16_r1 * 2**32 + u16_r2 * 2**48;
    pol SUM_96  = SUM_64     + u16_r3 * 2**64 + u16_r4 * 2**80;
    pol SUM_128 = SUM_96     + u16_r5 * 2**96 + u16_r6 * 2**112;

    // ========= ADDITION/SUBTRACTION Operation Constraints ===============================
    //
    // Addition and subtraction relations are very similar and will be consolidated.
    // For the addition we have to replace OP'(a,b) in the above relation by a+b and
    // for subtraction by a-b. Using operation selector values to toggle "+b" vs. "-b"
    // makes the deal with the adaptation that the carry term needs to be subtracted
    // instead of being added. To sumarize, for the first relation, addition needs to
    // satisfy:
    // sum_128 + carry * 2^128 - a - b = 0
    // while the subtraction satisfies:
    // sum_128 - carry * 2^128 - a + b = 0
    //
    // Finally, we would like this relation to also satisfy the addition over the finite field.
    // For this, we add c in the relation conditoned by the ff type selector ff_tag. We emphasize
    // that this relation alone for FF will not imply correctness of the FF addition. We only want
    // it to be satisfied. A separate relation will ensure correctness of it.
    //
    // The second relation will consist in showing that sum_N - c = 0 for N = 8, 16, 32, 64, 128.

    #[ALU_ADD_SUB_1]
    (op_add + op_sub) * (SUM_128 - ia + ff_tag * ic) + (op_add - op_sub) * (cf * 2**128 - ib) = 0;

    // Helper polynomial
    pol SUM_TAG = u8_tag * SUM_8 + u16_tag * SUM_16 + u32_tag * SUM_32 + u64_tag * SUM_64 + u128_tag * SUM_128;

    #[ALU_ADD_SUB_2]
    (op_add + op_sub) * (SUM_TAG + ff_tag * ia - ic) + ff_tag * (op_add - op_sub) * ib = 0;

    // ========= MULTIPLICATION Operation Constraints ===============================

    // ff multiplication
    #[ALU_MULTIPLICATION_FF]
    ff_tag * op_mul * (ia * ib - ic) = 0;

    // We need 2k bits to express the product (a*b) over the integer, i.e., for type uk
    // we express the product as sum_k (u8 is an exception as we need 8-bit registers)

    // We group relations for u8, u16, u32, u64 together.

    // Helper polynomial
    pol SUM_TAG_NO_128 = u8_tag * SUM_8 + u16_tag * SUM_16 + u32_tag * SUM_32 + u64_tag * SUM_64;

    #[ALU_MUL_COMMON_1]
    (1 - ff_tag - u128_tag) * op_mul * (SUM_128 - ia * ib) = 0;

    #[ALU_MUL_COMMON_2]
    op_mul * (SUM_TAG_NO_128 - (1 - ff_tag - u128_tag) * ic) = 0;

    // ========= u128 MULTIPLICATION Operation Constraints ===============================
    //
    // We express a, b in 64-bit slices: a = a_l + a_h * 2^64
    //                                   b = b_l + b_h * 2^64
    // We show that c satisfies: a_l * b_l + (a_h * b_l + a_l * b_h) * 2^64 = R * 2^128 + c
    // for a R < 2^65. Equivalently:
    // a * b_l + a_l * b_h * 2^64 = (CF * 2^64 + R_64) * 2^128 + c
    // for a bit carry flag CF and R_64 range constrained to 64 bits.
    // We use two lines in the execution trace. First line represents a 
    // as decomposed over 16-bit registers. Second line represents b.
    // Selector flag is only toggled in the first line and we access b through
    // shifted polynomials.
    // R_64 is stored in u16_r7, u16_r8, u16_r9, u_16_r10

    // 64-bit higher limb
    pol SUM_HIGH_64 = u16_r3 + u16_r4 * 2**16 + u16_r5 * 2**32 + u16_r6 * 2**48;

    // 64-bit lower limb for next row
    pol SUM_LOW_SHIFTED_64 = u8_r0' + u8_r1' * 2**8 + u16_r0' * 2**16 + u16_r1' * 2**32 + u16_r2' * 2**48;

    // 64-bit higher limb for next row
    pol SUM_HIGH_SHIFTED_64 = u16_r3' + u16_r4' * 2**16 + u16_r5' * 2**32 + u16_r6' * 2**48;

    // R_64 decomposition
    pol R_64 = u16_r7 + u16_r8 * 2**16 + u16_r9 * 2**32 + u16_r10 * 2**48;

    // Arithmetic relations
    u128_tag * op_mul * (SUM_64 + SUM_HIGH_64 * 2**64 - ia) = 0;
    u128_tag * op_mul * (SUM_LOW_SHIFTED_64 + SUM_HIGH_SHIFTED_64 * 2**64 - ib) = 0;
    #[ALU_MULTIPLICATION_OUT_U128]
    u128_tag * op_mul * (
            ia * SUM_LOW_SHIFTED_64
          + SUM_64 * SUM_HIGH_SHIFTED_64 * 2**64
          - (cf * 2**64 + R_64) * 2**128
          - ic
        ) = 0;

    // ========= BITWISE NOT Operation Constraints ===============================
    // Constrain mem_tag to not be FF (BITWISE NOT doesn't make sense for FF)
    // TODO decide if it is done here or in another trace

    // Do not allow ff_tag to be set if we are doing bitwise
    pol SEL_BITWISE = op_not; // Add more bitwise operations
    #[ALU_FF_NOT_XOR]
    SEL_BITWISE * ff_tag = 0;

    // The value 2^k - 1
    pol UINT_MAX = u8_tag * 2**8 + 
                u16_tag * 2**16 +
                u32_tag * 2**32 +
                u64_tag * 2**64 +
                u128_tag * 2**128 - 1;

    // BITWISE NOT relation is: a + ~a = 2^k - 1
    // Or (a + ~a - 2^k + 1) = 0;
    // value of "a" stored in ia and "~a" stored in ic
    #[ALU_OP_NOT]
    op_not * (ia + ic - UINT_MAX) = 0;

    // ========= EQUALITY Operation Constraints ===============================
    // TODO: Note this method differs from the approach taken for "equality to zero" checks
    // in handling the error tags found in main and mem files. The predicted relation difference
    // is minor and when we optimise we will harmonise the methods based on actual performance.

    // Equality of two elements is found by performing an "equality to zero" check.
    // This relies on the fact that the inverse of a field element exists for all elements except zero
    // 1) Given two values x & y, find the difference z = x - y
    // 2) If x & y are equal, z == 0 otherwise z != 0
    // 3) Field equality to zero can be done as follows
    //   a) z(e(x - w) + w) - 1 + e = 0;
    //   b) where w = z^-1 and e is a boolean value indicating if z == 0
    //   c) if e == 0; zw = 1 && z has an inverse. If e == 1; z == 0 and we set w = 0;

    // Registers Ia and Ib hold the values that equality is to be tested on
    pol DIFF = ia - ib;

    // Need an additional helper that holds the inverse of the difference;
    pol commit op_eq_diff_inv;

    // If EQ or sel_cmp selector is set, ic needs to be boolean
    #[ALU_RES_IS_BOOL]
    (sel_cmp + op_eq) * (ic * (1 - ic)) = 0;

    #[ALU_OP_EQ]
    op_eq * (DIFF * (ic * (1 - op_eq_diff_inv) + op_eq_diff_inv) - 1 + ic) = 0;

    // ========= LT/LTE Operation Constraints ===============================
    // There are two routines that we utilise as part of this LT/LTE check
    // (1) Decomposition into two 128-bit limbs, lo and hi respectively and a borrow (1 or 0);
    // (2) 128 bit-range checks when checking an arithmetic operation has not overflowed the field.

    // ========= COMPARISON OPERATION - EXPLANATIONS =================================================
    // To simplify the comparison circuit, we implement a GreaterThan(GT) circuit. This is ideal since
    // if we need a LT operation, we just swap the inputs and if we need the LTE operation, we just NOT the GT constraint
    // Given the inputs x, y and q where x & y are integers in the range [0,...,p-1] and q is the boolean result to the query (x > y).
    // Then there are two scenarios:
    //    (1) (x > y) -> x - y - 1 = result, where 0 <= result. i.e. the result does not underflow the field.
    //    (2)!(x > y) -> (x <= y) = y - x = result, where the same applies as above.

    // These conditions can be combined with the GT constraint, q (that x > y) as follows:
    // (x - y - 1) * q + (y - x) (1 - q) = result

    // If LT, then swap ia and ib else keep the same
    pol INPUT_IA  = op_lt * ib + (op_lte + op_cast) * ia;
    pol INPUT_IB  = op_lt * ia + op_lte * ib;

    pol commit borrow;
    pol commit a_lo;
    pol commit a_hi;
    // Check INPUT_IA is well formed from its lo and hi limbs
    #[INPUT_DECOMP_1]
    INPUT_IA = (a_lo + 2 ** 128 * a_hi) * (sel_cmp + op_cast);

    pol commit b_lo;
    pol commit b_hi;
    // Check INPUT_IB is well formed from its lo and hi limbs
    #[INPUT_DECOMP_2]
    INPUT_IB = (b_lo + 2 ** 128 * b_hi) * sel_cmp;

    pol commit p_sub_a_lo; // p_lo - a_lo
    pol commit p_sub_a_hi; // p_hi - a_hi
    pol commit p_a_borrow;
    p_a_borrow * (1 - p_a_borrow) = 0;

    // (p - 1) lower 128 bits = 53438638232309528389504892708671455232
    // (p - 1) upper 128 bits = 64323764613183177041862057485226039389 

    // Check that decomposition of a into lo and hi limbs do not overflow p.
    // This is achieved by checking a does not underflow p: (p_lo > a_lo && p_hi >= ahi) || (p_lo <= a_lo && p_hi > a_hi)
    // First condition is if borrow = 0, second condition is if borrow = 1
    // This underflow check is done by the 128-bit check that is performed on each of these lo and hi limbs.
    #[SUB_LO_1]
    (p_sub_a_lo  - (53438638232309528389504892708671455232 - a_lo + p_a_borrow * 2 ** 128)) * (sel_cmp + op_cast + op_div_std) = 0;
    #[SUB_HI_1]
    (p_sub_a_hi - (64323764613183177041862057485226039389 - a_hi - p_a_borrow)) * (sel_cmp + op_cast + op_div_std) = 0;

    pol commit p_sub_b_lo;
    pol commit p_sub_b_hi;
    pol commit p_b_borrow;
    p_b_borrow * (1 - p_b_borrow) = 0;

    // Check that decomposition of b into lo and hi limbs do not overflow/underflow p.
    // This is achieved by checking (p_lo > b_lo && p_hi >= bhi) || (p_lo <= b_lo && p_hi > b_hi)
    // First condition is if borrow = 0, second condition is if borrow = 1;
    #[SUB_LO_2]
    (p_sub_b_lo - (53438638232309528389504892708671455232 - b_lo + p_b_borrow * 2 ** 128)) * sel_cmp = 0;
    #[SUB_HI_2]
    (p_sub_b_hi - (64323764613183177041862057485226039389 - b_hi - p_b_borrow)) * sel_cmp = 0;

    // Calculate the combined relation: (a - b - 1) * q + (b -a ) * (1-q)
    // Check that (a > b) by checking (a_lo > b_lo && a_hi >= bhi) || (alo <= b_lo && a_hi > b_hi)
    // First condition is if borrow = 0, second condition is if borrow = 1;
    pol A_SUB_B_LO = a_lo - b_lo - 1 + borrow * 2 ** 128;
    pol A_SUB_B_HI = a_hi - b_hi - borrow;

    // Check that (a <= b) by checking (b_lo >= a_lo && b_hi >= a_hi) || (b_lo < a_lo && b_hi > a_hi)
    // First condition is if borrow = 0, second condition is if borrow = 1;
    pol B_SUB_A_LO = b_lo - a_lo + borrow * 2 ** 128;
    pol B_SUB_A_HI = b_hi - a_hi - borrow;


    // If this is a LT operation, we already swapped the inputs so the result of ic is still correct
    // If this is a LTE operation, we invert the value of ic.
    pol IS_GT = op_lt * ic + (1 - ic) * op_lte;

    // When IS_GT = 1, we enforce the condition that a > b and thus a - b - 1 does not underflow.
    // When IS_GT = 0, we enforce the condition that a <= b and thus b - a does not underflow.
    // ========= Analysing res_lo and res_hi scenarios for LTE =================================
    // (1) Assume a proof satisfies the constraints for LTE(x,y,1), i.e., x <= y
    //     Therefore ia = x, ib = y and ic = 1.
    //    (a) We do not swap the operands, so a = x and b = y,
    //    (b) IS_GT = 1 - ic = 0
    //    (c) res_lo = B_SUB_A_LO and res_hi = B_SUB_A_HI
    //    (d) res_lo = y_lo - x_lo + borrow * 2**128 and res_hi = y_hi - x_hi - borrow.
    //    (e) Due to 128-bit range checks on res_lo, res_hi, y_lo, x_lo, y_hi, x_hi, we
    //        have the guarantee that res_lo >= 0 && res_hi >= 0. Furthermore, borrow is
    //        boolean and so we have two cases to consider:
    //         (i)  borrow == 0 ==> y_lo >= x_lo && y_hi >= x_hi
    //         (ii) borrow == 1 ==> y_hi >= x_hi + 1 ==> y_hi > x_hi
    //        This concludes the proof as for both cases, we must have: y >= x
    //
    // (2) Assume a proof satisfies the constraints for LTE(x,y,0), i.e. x > y.
    //     Therefore ia = x, ib = y and ic = 0.
    //    (a) We do not swap the operands, so a = x and b = y,
    //    (b) IS_GT = 1 - ic = 1
    //    (c) res_lo = A_SUB_B_LO and res_hi = A_SUB_B_HI
    //    (d) res_lo = x_lo - y_lo - 1 + borrow * 2**128 and res_hi = x_hi - y_hi - borrow.
    //    (e) Due to 128-bit range checks on res_lo, res_hi, y_lo, x_lo, y_hi, x_hi, we
    //        have the guarantee that res_lo >= 0 && res_hi >= 0. Furthermore, borrow is
    //        boolean and so we have two cases to consider:
    //         (i)  borrow == 0 ==> x_lo > y_lo && x_hi >= y_hi
    //         (ii) borrow == 1 ==> x_hi > y_hi
    //        This concludes the proof as for both cases, we must have: x > y
    //

    // ========= Analysing res_lo and res_hi scenarios for LT ==================================
    // (1) Assume a proof satisfies the constraints for LT(x,y,1), i.e. x < y.
    //     Therefore ia = x, ib = y and ic = 1.
    //    (a) We DO swap the operands, so a = y and b = x,
    //    (b) IS_GT = ic = 1
    //    (c) res_lo = A_SUB_B_LO and res_hi = A_SUB_B_HI, **remember we have swapped inputs**
    //    (d) res_lo = y_lo - x_lo - 1 + borrow * 2**128 and res_hi = y_hi - x_hi - borrow.
    //    (e) Due to 128-bit range checks on res_lo, res_hi, y_lo, x_lo, y_hi, x_hi, we
    //        have the guarantee that res_lo >= 0 && res_hi >= 0. Furthermore, borrow is
    //        boolean and so we have two cases to consider:
    //         (i)  borrow == 0 ==> y_lo > x_lo && y_hi >= x_hi
    //         (ii) borrow == 1 ==> y_hi > x_hi
    //        This concludes the proof as for both cases, we must have: x < y
    //
    // (2) Assume a proof satisfies the constraint for LT(x,y,0), i.e. x >= y.
    //     Therefore ia = x, ib = y and ic = 0.
    //    (a) We DO swap the operands, so a = y and b = x,
    //    (b) IS_GT = ic = 0
    //    (c) res_lo = B_SUB_A_LO and res_hi = B_SUB_A_HI, **remember we have swapped inputs**
    //    (d) res_lo = a_lo - y_lo + borrow * 2**128 and res_hi = a_hi - y_hi - borrow.
    //    (e) Due to 128-bit range checks on res_lo, res_hi, y_lo, x_lo, y_hi, x_hi, we
    //        have the guarantee that res_lo >= 0 && res_hi >= 0. Furthermore, borrow is
    //        boolean and so we have two cases to consider:
    //         (i)  borrow == 0 ==> x_lo >= y_lo && x_hi >= y_hi
    //         (ii) borrow == 1 ==> x_hi > y_hi
    //        This concludes the proof as for both cases, we must have: x >= y

    pol commit res_lo;
    pol commit res_hi;
    #[RES_LO]
    (res_lo - (A_SUB_B_LO * IS_GT + B_SUB_A_LO * (1 - IS_GT))) * sel_cmp = 0;
    #[RES_HI]
    (res_hi - (A_SUB_B_HI * IS_GT + B_SUB_A_HI * (1 - IS_GT))) * sel_cmp = 0;

    // ========= RANGE OPERATIONS ===============================

    // Each call to LT/LTE requires 5x 256-bit range checks. We keep track of how many are left here.
    pol commit cmp_rng_ctr;

    // TODO: combine into 1 equation, left separate for debugging
    // the number of range checks must decrement by 1 until it is equal to 0;
    #[CMP_CTR_REL_1]
    (cmp_rng_ctr' - cmp_rng_ctr + 1) * cmp_rng_ctr = 0;
    // if this row is a comparison operation, the next range_check_remaining value is set to 4
    // it is not set to 5 since we do 1 as part of the comparison.
    #[CMP_CTR_REL_2]
    (cmp_rng_ctr' - 4) * sel_cmp = 0;
    
    sel_rng_chk * (1 - sel_rng_chk) = 0;
    // If we have remaining range checks, we cannot have sel_cmp set. This prevents malicious truncating of the range
    // checks by adding a new LT/LTE operation before all the range checks from the previous computation are complete.
    sel_rng_chk * sel_cmp = 0;

    // sel_rng_chk = 1 when cmp_rng_ctr != 0 and sel_rng_chk = 0 when cmp_rng_ctr = 0;
    #[CTR_NON_ZERO_REL]
    cmp_rng_ctr * ((1 - sel_rng_chk) * (1 -  op_eq_diff_inv) +  op_eq_diff_inv) - sel_rng_chk = 0;

    // We perform a range check if we have some range checks remaining or we are performing a comparison op
    pol RNG_CHK_OP = sel_rng_chk + sel_cmp + op_cast + op_cast_prev + shift_lt_bit_len + op_div;

    pol commit sel_rng_chk_lookup;
    // TODO: Possible optimisation here if we swap the op_shl and op_shr with shift_lt_bit_len.
    // Shift_lt_bit_len is a more restrictive form therefore we can avoid performing redundant range checks when we know the result == 0.
    #[RNG_CHK_LOOKUP_SELECTOR]
    sel_rng_chk_lookup' = sel_cmp' + sel_rng_chk' + op_add' + op_sub' + op_mul' + op_mul * u128_tag + op_cast' + op_cast_prev' + op_shl' + op_shr' + op_div';

    // Perform 128-bit range check on lo part
    #[LOWER_CMP_RNG_CHK]
    a_lo = SUM_128 * RNG_CHK_OP;

    // Perform 128-bit range check on hi part
    #[UPPER_CMP_RNG_CHK]
    a_hi = (u16_r7 + u16_r8 * 2**16 + 
           u16_r9 * 2**32 + u16_r10 * 2**48 + 
           u16_r11 * 2**64 + u16_r12 * 2**80 + 
           u16_r13 * 2**96 + u16_r14 * 2**112) * RNG_CHK_OP;

    // Shift all elements "across" by 2 columns
    // TODO: there is an optimisation where we are able to do 1 less range check as the range check on
    // P_SUB_B is implied by the other range checks.
    // Briefly: given a > b and p > a and p > a - b - 1, it is sufficient confirm that p > b without a range check
    // To accomplish this we would likely change the order of the range_check so we can skip p_sub_b
    #[SHIFT_RELS_0]
    (a_lo' - b_lo) * sel_rng_chk' = 0;
    (a_hi' - b_hi) * sel_rng_chk' = 0;
    #[SHIFT_RELS_1]
    (b_lo' - p_sub_a_lo) * sel_rng_chk' = 0;
    (b_hi' - p_sub_a_hi) * sel_rng_chk' = 0;
    #[SHIFT_RELS_2]
    (p_sub_a_lo' - p_sub_b_lo) * sel_rng_chk'= 0;
    (p_sub_a_hi' - p_sub_b_hi) * sel_rng_chk'= 0;
    #[SHIFT_RELS_3]
    (p_sub_b_lo' - res_lo) * sel_rng_chk'= 0;
    (p_sub_b_hi' - res_hi) * sel_rng_chk'= 0;

    // ========= CAST Operation Constraints ===============================
    // We handle the input ia independently of its tag, i.e., we suppose it can take
    // any value between 0 and p-1.
    // We decompose the input ia in 8-bit/16-bit limbs and prove that the decomposition
    // sums up to ia over the integers (i.e., no modulo p wrapping). To prove this, we
    // re-use techniques above from LT/LTE opcode. The following relations are toggled for CAST:
    // - #[INPUT_DECOMP_1] shows a = a_lo + 2 ** 128 * a_hi
    // - #[SUB_LO_1] and #[SUB_LO_1] shows that the above does not overflow modulo p.
    // - We toggle RNG_CHK_OP with op_cast to show that a_lo, a_hi are correctly decomposed
    //   over the 8/16-bit ALU registers in #[LOWER_CMP_RNG_CHK] and #[UPPER_CMP_RNG_CHK].
    // - The 128-bit range checks for a_lo, a_hi are activated in #[RNG_CHK_LOOKUP_SELECTOR].
    // - We copy p_sub_a_lo resp. p_sub_a_hi into next row in columns a_lo resp. a_hi so
    //   that decomposition into the 8/16-bit ALU registers and range checks are performed.
    //   Copy is done in #[OP_CAST_RNG_CHECK_P_SUB_A_LOW/HIGH] below.
    //   Activation of decomposition and range check is achieved by adding op_cast_prev in
    //   #[LOWER_CMP_RNG_CHK], #[UPPER_CMP_RNG_CHK] and #[RNG_CHK_LOOKUP_SELECTOR].
    // - Finally, the truncated result SUM_TAG is copied in ic as part of #[ALU_OP_CAST] below.
    // - Note that the tag of return value must be constrained to be in_tag and is enforced in
    //   the main and memory traces.
    //
    // TODO: Potential optimization is to un-toggle all CAST relevant operations when ff_tag is
    //       enabled. In this case, ic = ia trivially.
    //       Another one is to activate range checks in a more granular way depending on the tag.

    #[OP_CAST_PREV_LINE]
    op_cast_prev' = op_cast;

    #[ALU_OP_CAST]
    op_cast * (SUM_TAG + ff_tag * ia - ic) = 0;

    #[OP_CAST_RNG_CHECK_P_SUB_A_LOW]
    op_cast * (a_lo' - p_sub_a_lo) = 0;

    #[OP_CAST_RNG_CHECK_P_SUB_A_HIGH]
    op_cast * (a_hi' - p_sub_a_hi) = 0;

    // 128-bit multiplication and CAST need two rows in ALU trace. We need to ensure
    // that another ALU operation does not start in the second row.
    #[TWO_LINE_OP_NO_OVERLAP]
    (op_mul * u128_tag + op_cast) * sel_alu' = 0;

    // ========= SHIFT LEFT/RIGHT OPERATIONS ===============================
    // Given (1) an input b, within the range [0, 2**128-1], 
    //       (2) a value s, the amount of bits to shift b by,
    //       (3) and a memory tag, mem_tag that supports a maximum of t bits.
    // Split input b into Big Endian hi and lo limbs, (we re-use the b_hi and b_lo columns we used for the comparison operators)
    // b_hi and b_lo, and the number of bits represented by the memory tag, t.
    // If we are shifting by more than the bit length represented by the memory tag, the result is trivially zero
    //
    // === Steps when performing SHR
    // (1) Prove the correct decomposition: b_hi * 2**s + b_lo = b
    // (2) Range check b_hi < 2**(t-s) && b_lo < 2**s, ensure we have not overflowed the limbs during decomp
    // (3) Return b_hi
    //
    //  <--(t-s) bits --> |   <-- s bits -->
    // -------------------|-------------------
    // |      b_hi        |       b_lo       | --> b
    // ---------------------------------------
    //
    // === Steps when performing SHL
    // (1) Prove the correct decomposition: b_hi * 2**(t-s) + b_lo = b
    // (2) Range check b_hi < 2**s && b_lo < 2**(t-s)
    // (3) Return b_lo * 2**s
    // 
    //  <-- s bits -->   | <-- (t-s) bits -->
    // ------------------|-------------------
    // |      b_hi       |      b_lo        | --> b
    // --------------------------------------

    // Check that b_lo and b_hi are range checked such that:
    // SHR: b_hi < 2**(t - s) && b_lo < 2**s
    // SHL: b_hi < 2**s && b_lo < 2**(t - s)

    // In lieu of a variable length check, we can utilise 2 fixed range checks instead.
    // Given the dynamic range check of 0 <= b_hi < 2**(t-s), where s < t
    // (1) 0 <= b_hi < 2**128
    // (2) 0 <= 2**(t - s) - b_hi < 2**128
    // Note that (1) is guaranteed elsewhere through the tagged memory model, so we focus on (2) here. 

    // === General Notes:
    // There are probably ways to merge various relations for the SHL/SHR, but they are separate
    // now while we are still figuring out.

    
    // We re-use the a_lo and a_hi cols from the comparison operators for the range checks
    // SHR: (1) a_lo = 2**s - b_lo, (2) a_hi = 2**(t-s) - b_hi
    // === Range checks: (1) a_lo < 2**128, (2) a_hi < 2**128.
    #[SHR_RANGE_0]
    shift_lt_bit_len * op_shr * (a_lo - (two_pow_s - b_lo - 1)) = 0;
    #[SHR_RANGE_1]
    shift_lt_bit_len * op_shr * (a_hi - (two_pow_t_sub_s - b_hi - 1)) = 0;

    // SHL: (1) a_lo = 2**(t-s) - b_lo, (2) a_hi = 2**s - b_hi
    // === Range checks: (1) a_lo < 2**128, (2) a_hi < 2**128.
    #[SHL_RANGE_0]
    shift_lt_bit_len * op_shl * (a_lo - (two_pow_t_sub_s - b_lo - 1)) = 0;
    #[SHL_RANGE_1]
    shift_lt_bit_len * op_shl * (a_hi - (two_pow_s - b_hi - 1)) = 0;

    // Indicate if the shift amount < MAX_BITS
    pol commit shift_lt_bit_len;
    shift_lt_bit_len * (1 - shift_lt_bit_len) = 0;

    // The number of bits represented by the memory tag, any shifts greater than this will result in zero.
    pol MAX_BITS = u8_tag * 8 + 
                u16_tag * 16 +
                u32_tag * 32 +
                u64_tag * 64 +
                u128_tag * 128;

    // The result of MAX_BITS - ib, this used as part of the range check with the main trace
    pol commit t_sub_s_bits;

    // Lookups for powers of 2. 
    // 2**(MAX_BITS - ib), constrained as part of the range check to the main trace
    pol commit two_pow_t_sub_s;
    // 2 ** ib, constrained as part of the range check to the main trace
    pol commit two_pow_s; 

    // For our assumptions to hold, we must check that s < MAX_BITS. This can be achieved by the following relation.
    // We check if s <= MAX_BITS || s >= MAX_BITS using boolean shift_lt_bit_len. 
    // Regardless of which side is evaluated, the value of t_sub_s_bits < 2**8 
    // There is no chance of an underflow involving ib to result in a t_sub_b_bits < 2**8 ib is range checked to be < 2**8
    // The range checking of t_sub_b_bits in the range [0, 2**8) is done by the lookup for 2**t_sub_s_bits
    #[SHIFT_LT_BIT_LEN]
    t_sub_s_bits = sel_shift_which * (shift_lt_bit_len * (MAX_BITS - ib) + (1 - shift_lt_bit_len) * (ib - MAX_BITS));

    // ========= SHIFT RIGHT OPERATIONS ===============================
    // a_hi * 2**s + a_lo = a
    // If ib >= MAX_BITS, we trivially skip this check since the result will be forced to 0.
    #[SHR_INPUT_DECOMPOSITION]
    shift_lt_bit_len * op_shr * ((b_hi * two_pow_s + b_lo) - ia) = 0;

    // Return hi limb, if ib >= MAX_BITS, the output is forced to be 0
    #[SHR_OUTPUT]
    op_shr * (ic - (b_hi * shift_lt_bit_len)) = 0;

    // ========= SHIFT LEFT OPERATIONS ===============================
    // a_hi * 2**(t-s) + a_lo = a
    // If ib >= MAX_BITS, we trivially skip this check since the result will be forced to 0.
    #[SHL_INPUT_DECOMPOSITION]
    shift_lt_bit_len * op_shl * ((b_hi * two_pow_t_sub_s + b_lo) - ia) = 0;

    // Return lo limb a_lo * 2**s, if ib >= MAX_BITS, the output is forced to be 0
    #[SHL_OUTPUT]
    op_shl * (ic - (b_lo * two_pow_s * shift_lt_bit_len)) = 0;
   
    // ========= INTEGER DIVISION ===============================
    // Operands: ia contains the dividend, ib contains the divisor, and ic contains the quotient (i.e. the result).
    // All operands are restricted to be up to 128.
    // The logic for integer division is to assert the correctness of this relationship:
    // dividend - remainder = divisor * quotient ==> ia - remainder = ib * ic; where remainder < ib
    // We do this using the following steps
    // (1) The only non-trivial division is the situation where ia > ib && ib > 0
    //      (a) if ia == ib => ic = 1 and remainder = 0 --> we can handle this as part of the standard division
    //      (b) if ia < ib  => ic = 0 and remainder = ia --> isolating this case eliminates the risk of ia - remainder underflowing as remainder < ib < ia
    //      (c) if ib == 0  => error_tag = 1 --> Handled in main trace
    // (2) Given ib and ic are restricted to U128, at most ib * ic will produce a 256-bit number.
    // (3) We use the primality check from cmp to check that this product has not overflowed the field.
    //     The Primality check takes a field element as input and ouputs two 128-bit limbs.
    //     i.e. it checks that the field element, represented with two 128-bit limbs lies in [0, p).
    //      (a) Given x, PC(x) -> [x_lo, x_hi], where x_lo < 2**128 && x_hi < 2**128 && x == x_lo + x_hi * 2**128
    //      (b) Additionally produces a witness that the x < (p - 1)
    //          p_sub_x_lo = p_lo - x_lo + borrow * 2**128  < 2**128
    //          p_sub_x_hi = p_hi - x_hi - borrow           < 2**128
    //      (c) Range checks over 128-bits are applied to x_lo, x_hi, p_sub_x_lo, and p_sub_x_hi.

    // Range check the remainder < divisor.
    pol commit remainder;
    // The op_div boolean must be set based on which division case it is.
    op_div = op_div_std + op_div_a_lt_b;

    // ======= Handling ia < ib =====
    // Boolean if ia < ib ==> ic = 0;
    pol commit op_div_a_lt_b;
    op_div_a_lt_b * (1 - op_div_a_lt_b) = 0;
    // To show this, we constrain ib - ia - 1 to be within 128 bits.
    // Since we need a range check we use the existing a_lo column that is range checked over 128 bits.
    op_div_a_lt_b * (a_lo - (ib - ia - 1)) = 0;
    op_div_a_lt_b * ic  = 0; // ic = 0
    op_div_a_lt_b * (ia - remainder) = 0; // remainder = a, might not be needed.


    // ====== Handling ia >= ib =====
    pol commit op_div_std;
    op_div_std * (1 - op_div_std) = 0;
    pol commit divisor_lo; // b
    pol commit divisor_hi;
    op_div_std * (ib - divisor_lo - 2**64 * divisor_hi) = 0;
    pol commit quotient_lo; // c
    pol commit quotient_hi;
    op_div_std * (ic - quotient_lo - 2**64 * quotient_hi) = 0;

    // Multiplying the limbs gives us the following relations.
    // (1) divisor_lo * quotient_lo --> Represents the bottom 128 bits of the result, i.e. values between [0, 2**128).
    // (2) divisor_lo * quotient_hi + quotient_lo * divisor_hi --> Represents the middle 128 bits of the result, i.e. values between [2**64, 2**196)
    // (3) divisor_hi * quotient_hi --> Represents the topmost 128 bits of the result, i.e. values between [2**128, 2**256).

    // We simplify (2) by further decomposing it into two limbs of 64 bits and adding the upper 64 bit to (3)
    // divisor_lo * quotient_hi + quotient_lo * divisor_hi = partial_prod_lo + 2**64 * partial_prod_hi
    // Need to range check that these are 64 bits
    pol commit partial_prod_lo;
    pol commit partial_prod_hi;
    divisor_hi * quotient_lo + divisor_lo * quotient_hi = partial_prod_lo + 2**64 * partial_prod_hi;

    pol PRODUCT = divisor_lo * quotient_lo + 2**64 * partial_prod_lo + 2**128 * (partial_prod_hi + divisor_hi * quotient_hi);

    // a_lo and a_hi contains the hi and lo limbs of PRODUCT
    // p_sub_a_lo and p_sub_a_hi contain the primality checks
    #[ALU_PROD_DIV]
    op_div_std * (PRODUCT - (a_lo + 2 ** 128 * a_hi)) = 0;
    // Range checks already performed via a_lo and a_hi
    // Primality checks already performed above via p_sub_a_lo and p_sub_a_hi

    // Range check remainder < ib and put the value in b_hi, it has to fit into a 128 bit range check
    #[REMAINDER_RANGE_CHK]
    op_div_std * (b_hi - (ib - remainder - 1)) = 0;
    
    // We need to perform 3 x 256-bit range checks: (a_lo, a_hi), (b_lo, b_hi), and (p_sub_a_lo, p_sub_a_hi) 
    // One range check happens in-line with the division
    #[CMP_CTR_REL_3]
    (cmp_rng_ctr' - 2) * op_div_std = 0;
    
    // If we have more range checks left we cannot do more divisions operations that might truncate the steps
    sel_rng_chk * op_div_std = 0;

    // Check PRODUCT = ia - remainder
    #[DIVISION_RELATION]
    op_div_std * (PRODUCT - (ia - remainder)) = 0;

    // === DIVISION 64-BIT RANGE CHECKS
    // 64-bit decompositions and implicit 64-bit range checks for each limb,
    // TODO: We need extra slice registers because we are performing an additional 64-bit range check in the same row, look into re-using old columns or refactoring
    // range checks to be more modular.
    // boolean to account for the division-specific 64-bit range checks.
    pol commit sel_div_rng_chk; 
    sel_div_rng_chk * (1 - sel_div_rng_chk) = 0;
    // sel_div_rng_chk && sel_div_rng_chk' = 1 if op_div_std = 1
    sel_div_rng_chk * sel_div_rng_chk' = op_div_std;

    pol commit div_u16_r0;
    pol commit div_u16_r1;
    pol commit div_u16_r2;
    pol commit div_u16_r3;
    pol commit div_u16_r4;
    pol commit div_u16_r5;
    pol commit div_u16_r6;
    pol commit div_u16_r7;

    divisor_lo =  op_div_std * (div_u16_r0 + div_u16_r1 * 2**16 + div_u16_r2 * 2**32 + div_u16_r3 * 2**48);
    divisor_hi =  op_div_std * (div_u16_r4 + div_u16_r5 * 2**16 + div_u16_r6 * 2**32 + div_u16_r7 * 2**48);
    quotient_lo = op_div_std * (div_u16_r0' + div_u16_r1' * 2**16 + div_u16_r2' * 2**32 + div_u16_r3' * 2**48);
    quotient_hi = op_div_std * (div_u16_r4' + div_u16_r5' * 2**16 + div_u16_r6' * 2**32 + div_u16_r7' * 2**48);

    // We need an extra 128 bits to do 2 more 64-bit range checks. We use b_lo (128 bits) to store partial_prod_lo(64 bits) and partial_prod_hi(64 bits. 
    // Use a shift to access the slices (b_lo is moved into the alu slice registers on the next row anyways as part of the SHIFT_RELS_0 relations)
    pol NEXT_SUM_64_LO = u8_r0' + u8_r1' * 2**8 + u16_r0' * 2**16 + u16_r1' * 2**32 + u16_r2' * 2**48;
    pol NEXT_SUM_128_HI = u16_r3' + u16_r4' * 2**16 + u16_r5' * 2**32 + u16_r6' * 2**48;
    partial_prod_lo = op_div_std * NEXT_SUM_64_LO;
    partial_prod_hi = op_div_std * NEXT_SUM_128_HI;

    //====== Inter-table Shift Constraints (Lookups) ============================================
    // Currently only used for shift operations but can be generalised for other uses.

    // Lookup for 2**(ib)
    #[LOOKUP_POW_2_0]
    sel_shift_which {ib, two_pow_s} in main.sel_rng_8 {main.clk, powers.power_of_2};

    // Lookup for 2**(t-ib)
    #[LOOKUP_POW_2_1]
    sel_shift_which {t_sub_s_bits ,two_pow_t_sub_s} in main.sel_rng_8 {main.clk, powers.power_of_2};

    //====== Inter-table Constraints (Range Checks) ============================================
    // TODO: Investigate optimising these range checks. Handling non-FF elements should require less range checks.
    //       One can increase the granularity based on the operation and tag. In the most extreme case,
    //       a specific selector per register might be introduced.
    #[LOOKUP_U8_0]
    sel_rng_chk_lookup { u8_r0 } in main.sel_rng_8 { main.clk };

    #[LOOKUP_U8_1]
    sel_rng_chk_lookup { u8_r1 } in main.sel_rng_8 { main.clk };

    #[LOOKUP_U16_0]
    sel_rng_chk_lookup { u16_r0 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_1]
    sel_rng_chk_lookup { u16_r1 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_2]
    sel_rng_chk_lookup { u16_r2 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_3]
    sel_rng_chk_lookup { u16_r3 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_4]
    sel_rng_chk_lookup { u16_r4 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_5]
    sel_rng_chk_lookup { u16_r5 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_6]
    sel_rng_chk_lookup { u16_r6 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_7]
    sel_rng_chk_lookup { u16_r7 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_8]
    sel_rng_chk_lookup { u16_r8 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_9]
    sel_rng_chk_lookup { u16_r9 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_10]
    sel_rng_chk_lookup { u16_r10 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_11]
    sel_rng_chk_lookup { u16_r11 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_12]
    sel_rng_chk_lookup { u16_r12 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_13]
    sel_rng_chk_lookup { u16_r13 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_U16_14]
    sel_rng_chk_lookup { u16_r14 } in main.sel_rng_16 { main.clk };

    // ==== Additional row range checks for division
    #[LOOKUP_DIV_U16_0]
    sel_div_rng_chk { div_u16_r0 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_DIV_U16_1]
    sel_div_rng_chk { div_u16_r1 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_DIV_U16_2]
    sel_div_rng_chk { div_u16_r2 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_DIV_U16_3]
    sel_div_rng_chk { div_u16_r3 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_DIV_U16_4]
    sel_div_rng_chk { div_u16_r4 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_DIV_U16_5]
    sel_div_rng_chk { div_u16_r5 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_DIV_U16_6]
    sel_div_rng_chk { div_u16_r6 } in main.sel_rng_16 { main.clk };

    #[LOOKUP_DIV_U16_7]
    sel_div_rng_chk { div_u16_r7 } in main.sel_rng_16 { main.clk };
